{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/Em1BhwXxJtQIHsWfUeGg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyasri-062008/PROJECT/blob/main/pyautogui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unF0bgHlTox9"
      },
      "outputs": [],
      "source": [
        "import pyautogui\n",
        "import cv2\n",
        "import numpy as np\n",
        "import dlib\n",
        "import time\n",
        "\n",
        "# Define the screen resolution and aspect ratio\n",
        "SCREEN_WIDTH, SCREEN_HEIGHT = pyautogui.size()\n",
        "ASPECT_RATIO = SCREEN_WIDTH/SCREEN_HEIGHT\n",
        "\n",
        "# Initialize the eye detector using Dlib\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "# Initialize the camera capture\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Define the click threshold and timer\n",
        "CLICK_THRESHOLD = 5\n",
        "CLICK_TIMER = 0\n",
        "\n",
        "# Loop over the frames from the camera\n",
        "while True:\n",
        "    # Read the frame from the camera\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Convert the frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the grayscale frame\n",
        "    faces = detector(gray)\n",
        "\n",
        "    # Loop over the detected faces\n",
        "    for face in faces:\n",
        "        # Detect the landmarks in the face using the predictor\n",
        "        landmarks = predictor(gray, face)\n",
        "\n",
        "        # Extract the coordinates of the left and right eyes\n",
        "        left_eye_x = landmarks.part(36).x\n",
        "        left_eye_y = landmarks.part(36).y\n",
        "        right_eye_x = landmarks.part(45).x\n",
        "        right_eye_y = landmarks.part(45).y\n",
        "\n",
        "        # Calculate the horizontal and vertical gaze coordinates\n",
        "        gaze_x = (left_eye_x + right_eye_x) / 2\n",
        "        gaze_y = (left_eye_y + right_eye_y) / 2\n",
        "\n",
        "        # Normalize the gaze coordinates to the screen resolution\n",
        "        norm_gaze_x = int(gaze_x * SCREEN_WIDTH / frame.shape[1])\n",
        "        norm_gaze_y = int(gaze_y * SCREEN_HEIGHT / frame.shape[0])\n",
        "\n",
        "        # Move the cursor to the normalized gaze coordinates\n",
        "        pyautogui.moveTo(norm_gaze_x, norm_gaze_y)\n",
        "\n",
        "        # Check if the user is clicking by measuring the distance between the eyes\n",
        "        eye_dist = np.sqrt((left_eye_x - right_eye_x) ** 2 + (left_eye_y - right_eye_y) ** 2)\n",
        "        if eye_dist < CLICK_THRESHOLD:\n",
        "            CLICK_TIMER += 1\n",
        "        else:\n",
        "            if CLICK_TIMER > 0:\n",
        "                pyautogui.click()\n",
        "            CLICK_TIMER = 0\n",
        "\n",
        "        # Draw a circle around the gaze point for visualization\n",
        "        cv2.circle(frame, (int(gaze_x), int(gaze_y)), 5, (0, 255, 0), -1)\n",
        "\n",
        "    # Show the frame with the gaze point and the click threshold\n",
        "    cv2.putText(frame, f\"Click threshold: {CLICK_THRESHOLD}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "    cv2.imshow(\"Eye Tracker\", frame)\n",
        "\n",
        "    # Exit if the user presses the 'q' key\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the camera capture and close the window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ]
}